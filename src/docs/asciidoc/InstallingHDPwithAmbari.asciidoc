Installing Hortonworks HDP 2.2 on single-node VM using Ambari
=============================================================

== Preparations

First we need a VM to work with. Follow the instructions in link:PreparingVMforAmbari.asciidoc[]

== Installing Ambari Server

=== Set up Ambari YUM repo

Get the `ambari.repo` definition. 

For Ambari 1.7.0 use:

[source]
----
wget -nv https://public-repo-1.hortonworks.com/ambari/centos6/1.x/updates/1.7.0/ambari.repo -O /etc/yum.repos.d/ambari.repo
----

For Ambari 2.0.x use:

[source]
----
wget -nv https://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.0.1/ambari.repo -O /etc/yum.repos.d/ambari.repo
----

For Ambari 2.1.x use:

[source]
----
wget -nv https://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.1.2.1/ambari.repo -O /etc/yum.repos.d/ambari.repo
----

Check that we have the Ambari repo listed as part of YUM repos:

[source]
----
yum repolist
----

=== Install Ambari server packages

Run the YUM install command (answer Y to all prompts):

[source]
----
yum install ambari-server
----

=== Configure and start the Ambari server

Run the setup command:

[source]
----
ambari-server setup
----

Now, start the Ambari server:

[source]
----
ambari-server start
----

You can now connect to Ambari server on port 8080, username is `admin` and password is `admin`.


== Create Hadoop Cluster

From the Ambari Dashboard click the `Launch Install Wizard` link.

I configured my single-node VMs hostname as the only host.

For the SSH Private key I used the one we generated on the VM. Just run this and cut-and-paste the key:

[source]
----
# cat .ssh/id_dsa
----

I ignored the warning about my hostname not beeing a FQDN.

For services to install with Ambari 1.7.0 I picked "HDFS", "YARN + MapReduce2", "Nagios", "Ganglia", "Kafka" and "ZooKeeper". For Ambari 2.0.x I picked "HDFS", "YARN + MapReduce2", "Ambari Metrics", "Kafka" and "ZooKeeper".  Everything ends up on the single node. 

When using Ambari 1.7.0 you will need to set password and alert email for Nagios. With Ambari 2.0.x I had to ignore the message saying I did not have enough RAM for the selected cpmponents.

The install should succeed and you should have an operational cluster.

==== Fix MapReduce warning when using Ambari 1.7.0

For Ambari 1.7.0 install there was 1 warning - "MapReduce2 Service Check" (File does not exist: hdfs://host:8020/hdp/apps/2.2.6.0-2800/mapreduce/mapreduce.tar.gz)

To overcome this, do the following:

Determine the exact version of HDP:

[source]
----
# readlink -f /usr/hdp/current/hadoop-client
/usr/hdp/2.2.6.0-2800/hadoop
----

Use the full version number that is displayed in that path.

.for HDP version 2.2.4.2-2
[source]
----
# HADOOP_USER_NAME=hdfs hdfs dfs -mkdir -p /hdp/apps/2.2.4.2-2/mapreduce/
# HADOOP_USER_NAME=hdfs hdfs dfs -copyFromLocal /usr/hdp/2.2.4.2-2/hadoop/mapreduce.tar.gz /hdp/apps/2.2.4.2-2/mapreduce/
----

.for HDP version 2.2.6.0-2800
[source]
----
# HADOOP_USER_NAME=hdfs hdfs dfs -mkdir -p /hdp/apps/2.2.6.0-2800/mapreduce/
# HADOOP_USER_NAME=hdfs hdfs dfs -copyFromLocal /usr/hdp/2.2.6.0-2800/hadoop/mapreduce.tar.gz /hdp/apps/2.2.6.0-2800/mapreduce/
----

The Hadoop cluster should now be operational.
